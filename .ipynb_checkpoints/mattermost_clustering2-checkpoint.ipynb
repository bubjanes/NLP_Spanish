{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with scikit-learn and kmeans \n",
    "## NLTK for preprocessing; scikit learn for vectorizing, matrix; k-means for clustering\n",
    "## This is our second attempt to apply ML algorithms with the data set from our client. The results a little better. \n",
    "scikit learn (version 0.20.2); nltk (version 3.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mpld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "#import mpld3\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"spanish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain a tokenizer function to use in our vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparar funciones de procesamiento de texto\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "            \n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add words to stopwords list that we want the ML to ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de \"stopwords\"\n",
    "stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "stopwords.append('https')\n",
    "stopwords.append('rt')\n",
    "stopwords.append('//t.co/86i0lev9kv')\n",
    "stopwords.append('hola')\n",
    "stopwords.append('Hola')\n",
    "stopwords.append('de')\n",
    "stopwords.append('del')\n",
    "stopwords.append('gracias')\n",
    "stopwords.append('muchas')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('new_mattermost.csv')\n",
    "x = df.groupby('UserName')\n",
    "\n",
    "clients = x.get_group('whatsapp')\n",
    "#print(type(clients))\n",
    "rawmess = clients.Mensaje.dropna()\n",
    "\n",
    "no_newlines = []\n",
    "for line in rawmess:\n",
    "    line = str(line)\n",
    "    line = line.split('\\n')\n",
    "    no_newlines.append(line)\n",
    "text = []\n",
    "for var in no_newlines:\n",
    "    for content in var:\n",
    "        if content.isdigit():\n",
    "            continue \n",
    "        if re.search(\"the\",content):\n",
    "            continue\n",
    "        if re.search(\"channel\",content):\n",
    "            continue\n",
    "        if re.search('/www.+?',content):\n",
    "            continue\n",
    "        if re.search('http',content):\n",
    "            continue\n",
    "        if re.search('buen dia',content):\n",
    "            continue\n",
    "        if re.search('turno gratis',content):\n",
    "            continue    \n",
    "        if content == \"\":\n",
    "            continue\n",
    "        text.append(content)\n",
    "print(type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pd.Series(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the term frequency-inverse document frequency matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(binary=True, max_df=.95,\n",
    "                                 min_df=15, stop_words=stopwords,\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform our vector object into a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/diplodatos-ayv/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'graci', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hol', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosostr', 'vuestr'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La matrix tiene 6151 filas (documentos) y 302 columnas (palabras)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts.astype('U'))\n",
    "print(\"La matrix tiene %i filas (documentos) y %i columnas (palabras)\\n\" % tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay en total 302 palabras:\n",
      "\n",
      "['abon', 'aca', 'adar', 'agend', 'ahi', 'ahor', 'algun', 'arañit', 'asegur', 'asi', 'asist', 'atiend', 'aut', 'avis', 'años', 'barri', 'bien', 'buen', 'buen dia', 'buen dias', 'buen tard', 'cambi', 'campañ', 'campañ gratuit', 'campañ gratuit medicin', 'campañ gratuit medicin estet', 'capital', 'carg', 'carl', 'cas', 'centr', 'claudi', 'cobertur', 'cobr', 'com', 'comprob', 'comunic', 'confirm', 'consult', 'consult par', 'contest', 'cordob', 'corre', 'corre electron', 'cost', 'cuand', 'cuant', 'cuot', 'd', 'dal', 'dar', 'dat', 'deb', 'debit', 'desd', 'despu', 'despues', 'deteccion', 'deteccion lunar', 'dia', 'dias', 'direccion', 'disculp', 'dni', 'domicili', 'dond', 'dos', 'dr', 'dr.', 'dra', 'electron', 'entonc', 'envi', 'envi link', 'esper', 'estan', 'estet', 'estudi', 'falt', 'favor', 'fech', 'fil', 'fil type', 'fil type not', 'fil type not support', 'fil type not support yet', 'flebolog', 'flebologi', 'fot', 'gabriel', 'gmail.com', 'gnc', 'graci', 'gratis', 'gratuit', 'gratuit medicin', 'gratuit medicin estet', 'gru', 'gust', 'hac', 'hac consult', 'hag', 'hast', 'hol', 'hol buen', 'hol buen dia', 'hol buen dias', 'hol buen tard', 'hol pagin', 'hol pagin web', 'hol pagin web quier', 'hol pagin web quier hac', 'hor', 'horari', 'hotmail.com', 'hoy', 'hs', 'igual', 'inform', 'interes', 'ir', 'juev', 'link', 'link pag', 'link par', 'link par pag', 'list', 'llam', 'lleg', 'llev', 'lun', 'lunar', 'm', 'mand', 'mar', 'mari', 'mart', 'mas', 'may', 'mañan', 'medi', 'medic', 'medicin', 'medicin estet', 'mejor', 'mes', 'miercol', 'mil', 'mism', 'model', 'mot', 'much', 'much graci', 'muchisim', 'muchisim graci', 'nacimient', 'nad', 'nadi', 'necesit', 'necesit turn', 'nombr', 'not', 'not support', 'not support yet', 'nuev', 'numer', 'ok', 'ok graci', 'ok.', 'pag', 'pag segur', 'pagin', 'pagin web', 'pagin web quier', 'pagin web quier hac', 'pagin web quier hac consult', 'par', 'par campañ', 'par consult', 'par cuand', 'par deteccion', 'par deteccion lunar', 'par dia', 'par flebologi', 'par lun', 'par pag', 'part', 'pas', 'patent', 'patrici', 'ped', 'per', 'perdon', 'perfect', 'pid', 'piern', 'pod', 'podr', 'podr ser', 'podri', 'poliz', 'porqu', 'posibl', 'pregunt', 'primer', 'problem', 'pued', 'pued pag', 'pued ser', 'q', 'qued', 'quer', 'quer sab', 'queri', 'quier', 'quier hac', 'quier hac consult', 'quier turn', 'quier turn campañ', 'quier turn campañ gratuit', 'quier turn campañ gratuit medicin', 'quier turn par', 'quier turn par deteccion', 'quier turn par deteccion lunar', 'quis', 'quis sab', 'quis turn', 'quis turn par', 'realiz', 'recib', 'remis', 'renov', 'respond', 'sab', 'sab si', 'sac', 'sac turn', 'sal', 'segur', 'segur par', 'seman', 'ser', 'seri', 'servici', 'si', 'si favor', 'si graci', 'si pued', 'si pued ser', 'si si', 'sig', 'sobr', 'sol', 'solicit', 'solicit turn', 'support', 'support yet', 'tambien', 'tard', 'tarjet', 'ten', 'tendr', 'teng', 'teng q', 'teng turn', 'tien', 'tod', 'tom', 'trabaj', 'tratamient', 'turn', 'turn campañ', 'turn campañ gratuit', 'turn campañ gratuit medicin', 'turn campañ gratuit medicin estet', 'turn par', 'turn par campañ', 'turn par deteccion', 'turn par deteccion lunar', 'type', 'type not', 'type not support', 'type not support yet', 'usted', 'varic', 'ver', 'vi', 'vien', 'viern', 'vill', 'voy', 'voy pod', 'web', 'web quier', 'web quier hac', 'web quier hac consult', 'x', 'x favor', 'x mañan', 'x tard', 'xq', 'yet']\n"
     ]
    }
   ],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "print(\"Hay en total %i palabras:\\n\" % len(terms))\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k_means: form clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 7\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cluster 0 tiene 44 elementos\n",
      "El cluster 1 tiene 54 elementos\n",
      "El cluster 2 tiene 574 elementos\n",
      "El cluster 3 tiene 4267 elementos\n",
      "El cluster 4 tiene 313 elementos\n",
      "El cluster 5 tiene 418 elementos\n",
      "El cluster 6 tiene 481 elementos\n"
     ]
    }
   ],
   "source": [
    "# Recuento del número de elementos en cada cluster\n",
    "for i in range(num_clusters):\n",
    "    print ('El cluster %i tiene %i elementos' % (i, clusters.count(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "[[ Cluster 0 ]]\n",
      "\n",
      "   WORDS /// domicili / mari / barri / fech / vill / dni / \n",
      "\n",
      "\n",
      "[[ Cluster 1 ]]\n",
      "\n",
      "   WORDS /// par deteccion lunar / par deteccion / turn par deteccion / turn par deteccion lunar / deteccion / deteccion lunar / \n",
      "\n",
      "\n",
      "[[ Cluster 2 ]]\n",
      "\n",
      "   WORDS /// par / turn / turn par / dia / par flebologi / flebologi / \n",
      "\n",
      "\n",
      "[[ Cluster 3 ]]\n",
      "\n",
      "   WORDS /// buen / hol / tard / flebologi / pued / si / \n",
      "\n",
      "\n",
      "[[ Cluster 4 ]]\n",
      "\n",
      "   WORDS /// si / bien / quier / pas / mañan / varic / \n",
      "\n",
      "\n",
      "[[ Cluster 5 ]]\n",
      "\n",
      "   WORDS /// quier turn / turn campañ gratuit / turn campañ / quier turn campañ / quier turn campañ gratuit / campañ gratuit / \n",
      "\n",
      "\n",
      "[[ Cluster 6 ]]\n",
      "\n",
      "   WORDS /// graci / much graci / much / ok / ok graci / buen / \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]     \n",
    "        \n",
    "for i in range(num_clusters):\n",
    "    print(\"[[ Cluster %d ]]\" % i, end='\\n\\n')\n",
    "    \n",
    "    print(\"   WORDS /// \", end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        print(terms[ind], end=' / ')\n",
    "    print('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
